{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing if lagged features were the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.optimize import minimize\n",
    "from kalman_filter.kalman_filter import (\n",
    "    ConstantVelocityKalmanFilter, FinancialModelKalmanFilter, optimize_kalman_hyperparameters\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pywt  # Ensure you have pywavelets installed for wavelet transforms\n",
    "# from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Hyperparameter Configurations\n",
    "# -----------------\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "LASSO_PARAM_GRID = {\"logisticregression__C\": np.logspace(-3, 2, 10)}\n",
    "RF_PARAM_GRID = {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]}\n",
    "XGB_PARAM_GRID = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "NN_PARAM_GRID = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100]\n",
    "}\n",
    "LSTM_PARAM_GRID = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100]\n",
    "}\n",
    "\n",
    "# Kalman Filter Hyperparameters\n",
    "CVKF_PARAM_GRID = [\n",
    "    {\"initial_state\": np.array([0.0]), \"Q_diag\": [q], \"R_diag\": [r]}\n",
    "    for q in [0.01, 0.1, 1.0, 10.0]\n",
    "    for r in [0.01, 0.1, 1.0, 10.0]\n",
    "]\n",
    "FMKF_PARAM_GRID = [\n",
    "    {\"initial_state\": np.array([0.0]), \"Q_diag\": [q], \"R_diag\": [r], \"alpha\": [a], \"beta\": [b]}\n",
    "    for q in [0.01, 0.1, 1.0, 10.0]\n",
    "    for r in [0.01, 0.1, 1.0, 10.0]\n",
    "    for a in [0.4, 0.6, 0.8, 1.0]\n",
    "    for b in [0.05, 0.1, 0.2, 0.4]\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Utility Functions\n",
    "# -----------------\n",
    "\n",
    "\n",
    "def five_way_split(X, y, train_size=0.5, val1_size=0.15, val2_size=0.1, kalman_size=0.1, test_size=0.15):\n",
    "    \"\"\"Split data into five subsets.\"\"\"\n",
    "    total_len = len(X)\n",
    "\n",
    "    train_len = round(total_len * train_size)\n",
    "    val1_len = round(total_len * val1_size)\n",
    "    val2_len = round(total_len * val2_size)\n",
    "    kalman_len = round(total_len * kalman_size)\n",
    "    test_len = total_len - train_len - val1_len - val2_len - kalman_len\n",
    "\n",
    "    train_idx = range(0, train_len)\n",
    "    val1_idx = range(train_len, train_len + val1_len)\n",
    "    val2_idx = range(train_len + val1_len, train_len + val1_len + val2_len)\n",
    "    kalman_idx = range(train_len + val1_len + val2_len, train_len + val1_len + val2_len + kalman_len)\n",
    "    test_idx = range(train_len + val1_len + val2_len + kalman_len, total_len)\n",
    "\n",
    "    return (\n",
    "        X.iloc[train_idx], X.iloc[val1_idx], X.iloc[val2_idx], X.iloc[kalman_idx], X.iloc[test_idx],\n",
    "        y.iloc[train_idx], y.iloc[val1_idx], y.iloc[val2_idx], y.iloc[kalman_idx], y.iloc[test_idx]\n",
    "    )\n",
    "\n",
    "\n",
    "def optimize_model_hyperparameters(model_fn, param_grid, X_train, y_train, validation_data, n_jobs=1):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter optimization using GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        model_fn: A callable that returns an instance of the model.\n",
    "        param_grid: Dictionary of hyperparameters to search.\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        validation_data: Tuple (X_val, y_val) for validation.\n",
    "        n_jobs: Number of parallel jobs for GridSearchCV.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The best model after GridSearchCV.\n",
    "        best_params: The best parameters from the search.\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics including Accuracy, Precision, Recall, F1, and AUC.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_pred_proba (array-like, optional): Predicted probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    if y_pred_proba is not None:\n",
    "        metrics[\"AUC\"] = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def preprocess_data_with_advanced_features(data_frame, target_column, lag_steps=None, rolling_window=10):\n",
    "    \"\"\"\n",
    "    Preprocess data for time series modeling with advanced feature engineering.\n",
    "    Ensures no data leakage by strictly using past and current data for feature generation.\n",
    "\n",
    "    Args:\n",
    "        data_frame (str): Variable name of loaded pandas data frame.\n",
    "        target_column (str): Target column name.\n",
    "        lag_steps (list): List of lag steps for feature engineering.\n",
    "        rolling_window (int): Window size for rolling features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature DataFrame (X) and target series (y).\n",
    "    \"\"\"\n",
    "    # Load data and parse dates\n",
    "    data = data_frame\n",
    "    data.index = pd.to_datetime(data.index, errors='coerce')  # Ensure index is datetime\n",
    "    assert data.index.is_monotonic_increasing, \"Dataset is not sorted by time.\"\n",
    "\n",
    "    # Fill missing values in the target column\n",
    "    data[target_column] = data[target_column].interpolate(method='linear').bfill()\n",
    "\n",
    "    # Initialize feature storage\n",
    "    features = []\n",
    "    indices = []\n",
    "\n",
    "    for end_idx in range(rolling_window, len(data)):\n",
    "        # Define the current window\n",
    "        window = data.iloc[end_idx - rolling_window:end_idx]\n",
    "\n",
    "        # Compute features for the current timestamp\n",
    "        current_features = {}\n",
    "\n",
    "        # Rolling statistics\n",
    "        signal_cols = [col for col in data.columns if col not in ['patient', 'newtest', 'target', 'event1', 'event2', 'event3', 'event4', 'sleepstage']]  # sleepstage excluded as categorical variable\n",
    "        for col in signal_cols:\n",
    "            current_features[f'{col}_roll_mean'] = window[col].mean()\n",
    "            current_features[f'{col}_roll_std'] = window[col].std()\n",
    "\n",
    "        \n",
    "        \n",
    "        # # Lagged features\n",
    "        # if lag_steps:\n",
    "        #     for lag in lag_steps:\n",
    "        #         if end_idx - lag >= 0:\n",
    "        #             current_features[f'{target_column}_lag{lag}'] = data[target_column].iloc[end_idx - lag]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Fourier Transform Features\n",
    "        for col in signal_cols:\n",
    "            fourier_transform = np.abs(np.fft.fft(window[col].fillna(0)))\n",
    "            current_features[f'{col}_fft_max'] = np.max(fourier_transform)\n",
    "            current_features[f'{col}_fft_mean'] = np.mean(fourier_transform)\n",
    "\n",
    "        # Wavelet Transform Features\n",
    "        for col in signal_cols:\n",
    "            coeffs = pywt.wavedec(window[col].fillna(0), 'db1', level=3)\n",
    "            current_features[f'{col}_wavelet_approx'] = coeffs[0].mean()\n",
    "            current_features[f'{col}_wavelet_detail1'] = coeffs[1].mean()\n",
    "            current_features[f'{col}_wavelet_detail2'] = coeffs[2].mean()\n",
    "\n",
    "        # Add features and corresponding index\n",
    "        features.append(current_features)\n",
    "        indices.append(data.index[end_idx])\n",
    "\n",
    "    # Convert features to DataFrame\n",
    "    feature_df = pd.DataFrame(features, index=indices)\n",
    "\n",
    "    # Align target values\n",
    "    y = data.loc[feature_df.index, target_column]\n",
    "\n",
    "    return feature_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load and Preprocess Data\n",
    "# -----------------\n",
    "\n",
    "def load_and_preprocess_data(dataframe):\n",
    "    # Load and preprocess data with advanced features\n",
    "    X, y = preprocess_data_with_advanced_features(\n",
    "        data_frame=dataframe,\n",
    "        target_column='target',\n",
    "        lag_steps=[1, 2, 3],\n",
    "        rolling_window=10\n",
    "    )\n",
    "\n",
    "    # Perform five-way split\n",
    "    X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test = five_way_split(\n",
    "        X, y, train_size=0.5, val1_size=0.15, val2_size=0.05, kalman_size=0.1, test_size=0.2\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "master_df = pd.read_stata('./data/processed-data/combined-patient-data-1_00.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group master dataframe by 'patient' and 'newtest' pairs (i.e. by each unique patient data)\n",
    "# Access or initialize each dataframe like: group_dict[('pid100100', 0)]\n",
    "group_dict = {\n",
    "    (val1, val2): data\n",
    "    for (val1, val2), data in master_df.groupby(['patient', 'newtest'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframes\n",
    "for group_key, subset_df in group_dict.items():\n",
    "    subset_df['target'] = subset_df[['event1', 'event2', 'event3', 'event4']].apply(lambda x: 1 if 'Hypopnea' in x.values or 'Apnea Obstructive' in x.values or 'Apnea Central' in x.values or 'Apnea Mixed' in x.values else 0, axis=1)\n",
    "\n",
    "    cols = list(subset_df.columns)\n",
    "    cols.remove('target')\n",
    "    cols.insert(3, 'target')\n",
    "    subset_df = subset_df[cols]\n",
    "\n",
    "    subset_df.set_index('timess', inplace=True)\n",
    "    \n",
    "    group_dict[group_key] = subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ideal_group_dict from group_dict using ideal_patients\n",
    "ideal_patients = pd.read_csv('./data/ideal_patients_1.csv')\n",
    "ideal_patients = list(ideal_patients.itertuples(index=False, name=None))\n",
    "\n",
    "ideal_group_dict = {\n",
    "    (val1, val2): group_dict[(val1, val2)]\n",
    "    for (val1, val2) in ideal_patients\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_before_dropnan = load_and_preprocess_data(ideal_group_dict[('pid100816', 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaN from Xs and corresponding rows from ys\n",
    "def drop_nan_from_Xy(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    # Drop rows with NaN from X\n",
    "    X_train_cleaned = X_train.dropna()\n",
    "    X_val1_cleaned = X_val1.dropna()\n",
    "    X_val2_cleaned = X_val2.dropna()\n",
    "    X_kalman_cleaned = X_kalman.dropna()\n",
    "    X_test_cleaned = X_test.dropna()\n",
    "    \n",
    "    # Drop corresponding rows from y\n",
    "    y_train_cleaned = y_train[X_train_cleaned.index]\n",
    "    y_val1_cleaned = y_val1[X_val1_cleaned.index]\n",
    "    y_val2_cleaned = y_val2[X_val2_cleaned.index]\n",
    "    y_kalman_cleaned = y_kalman[X_kalman_cleaned.index]\n",
    "    y_test_cleaned = y_test[X_test_cleaned.index]\n",
    "    \n",
    "    return (\n",
    "        X_train_cleaned, X_val1_cleaned, X_val2_cleaned, X_kalman_cleaned, X_test_cleaned,\n",
    "        y_train_cleaned, y_val1_cleaned, y_val2_cleaned, y_kalman_cleaned, y_test_cleaned\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test = drop_nan_from_Xy(*Xy_before_dropnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics: {'Accuracy': 0.7189668532070599, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.47334738591970743)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Logistic Regression\n",
    "# -----------------\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression(class_weight='balanced', max_iter=2000))  # Handle class imbalance\n",
    "])\n",
    "log_reg_grid = GridSearchCV(log_reg_pipeline, LASSO_PARAM_GRID, cv=5, scoring='roc_auc')\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "log_reg_model = log_reg_grid.best_estimator_\n",
    "\n",
    "log_reg_preds = log_reg_model.predict(X_test)\n",
    "log_reg_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, log_reg_preds),\n",
    "    \"Precision\": precision_score(y_test, log_reg_preds, zero_division=0),  # Avoid warning\n",
    "    \"Recall\": recall_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, log_reg_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Logistic Regression Metrics:\", log_reg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics: {'Accuracy': 0.977184674989238, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.8049222708004322)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heewon/Dev/Projects/OSA/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Random Forest\n",
    "# -----------------\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('randomforest', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced'))  # Handle class imbalance\n",
    "])\n",
    "rf_param_grid = {\n",
    "    \"randomforest__n_estimators\": [50, 100, 200],  # Prefixed by 'randomforest__'\n",
    "    \"randomforest__max_depth\": [None, 10, 20]\n",
    "}\n",
    "rf_grid = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='roc_auc')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_model = rf_grid.best_estimator_\n",
    "\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, rf_preds),\n",
    "    \"Precision\": precision_score(y_test, rf_preds),\n",
    "    \"Recall\": recall_score(y_test, rf_preds),\n",
    "    \"F1\": f1_score(y_test, rf_preds),\n",
    "    \"AUC\": roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Random Forest Metrics:\", rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Metrics: {'Accuracy': 0.977184674989238, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.7774871515252265)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# XGBoost\n",
    "# -----------------\n",
    "# Define the pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Prefix all hyperparameters for 'xgb' step with 'xgb__'\n",
    "xgb_param_grid = {\n",
    "    \"xgb__n_estimators\": [50, 100, 200],\n",
    "    \"xgb__max_depth\": [3, 5, 7],\n",
    "    \"xgb__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"xgb__subsample\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "xgb_grid = GridSearchCV(xgb_pipeline, xgb_param_grid, cv=5, scoring='roc_auc')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "# Predictions and metrics\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, xgb_preds),\n",
    "    \"Precision\": precision_score(y_test, xgb_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, xgb_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, xgb_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"XGBoost Metrics:\", xgb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Lasso (Base)\n",
    "# -----------------\n",
    "lasso_base_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling for consistent input\n",
    "    ('lasso', LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear', max_iter=2000))  # L1 Regularization\n",
    "])\n",
    "lasso_base_param_grid = {\"lasso__C\": np.logspace(-3, 2, 10)}  # Regularization strength\n",
    "\n",
    "lasso_base_grid = GridSearchCV(lasso_base_pipeline, lasso_base_param_grid, cv=5, scoring='roc_auc')\n",
    "lasso_base_grid.fit(X_train, y_train)\n",
    "lasso_base_model = lasso_base_grid.best_estimator_  # Capture the best Lasso model\n",
    "\n",
    "# Predictions and Metrics\n",
    "lasso_base_preds_proba = lasso_base_model.predict_proba(X_test)[:, 1]\n",
    "lasso_base_preds = (lasso_base_preds_proba > 0.5).astype(int)\n",
    "\n",
    "lasso_base_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, lasso_base_preds),\n",
    "    \"Precision\": precision_score(y_test, lasso_base_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, lasso_base_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, lasso_base_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, lasso_base_preds_proba)\n",
    "}\n",
    "print(\"Lasso (Base) Metrics:\", lasso_base_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
