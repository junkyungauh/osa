{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.optimize import minimize\n",
    "from kalman_filter.kalman_filter import (\n",
    "    ConstantVelocityKalmanFilter, FinancialModelKalmanFilter, optimize_kalman_hyperparameters\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pywt  # Ensure you have pywavelets installed for wavelet transforms\n",
    "# from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Hyperparameter Configurations\n",
    "# -----------------\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "LASSO_PARAM_GRID = {\"logisticregression__C\": np.logspace(-3, 2, 10)}\n",
    "RF_PARAM_GRID = {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]}\n",
    "XGB_PARAM_GRID = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "NN_PARAM_GRID = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100]\n",
    "}\n",
    "LSTM_PARAM_GRID = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100]\n",
    "}\n",
    "\n",
    "# Kalman Filter Hyperparameters\n",
    "CVKF_PARAM_GRID = [\n",
    "    {\"initial_state\": np.array([0.0]), \"Q_diag\": [q], \"R_diag\": [r]}\n",
    "    for q in [0.01, 0.1, 1.0, 10.0]\n",
    "    for r in [0.01, 0.1, 1.0, 10.0]\n",
    "]\n",
    "FMKF_PARAM_GRID = [\n",
    "    {\"initial_state\": np.array([0.0]), \"Q_diag\": [q], \"R_diag\": [r], \"alpha\": [a], \"beta\": [b]}\n",
    "    for q in [0.01, 0.1, 1.0, 10.0]\n",
    "    for r in [0.01, 0.1, 1.0, 10.0]\n",
    "    for a in [0.4, 0.6, 0.8, 1.0]\n",
    "    for b in [0.05, 0.1, 0.2, 0.4]\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Utility Functions\n",
    "# -----------------\n",
    "\n",
    "\n",
    "def five_way_split(X, y, train_size=0.5, val1_size=0.15, val2_size=0.1, kalman_size=0.1, test_size=0.15):\n",
    "    \"\"\"Split data into five subsets.\"\"\"\n",
    "    total_len = len(X)\n",
    "\n",
    "    train_len = round(total_len * train_size)\n",
    "    val1_len = round(total_len * val1_size)\n",
    "    val2_len = round(total_len * val2_size)\n",
    "    kalman_len = round(total_len * kalman_size)\n",
    "    test_len = total_len - train_len - val1_len - val2_len - kalman_len\n",
    "\n",
    "    train_idx = range(0, train_len)\n",
    "    val1_idx = range(train_len, train_len + val1_len)\n",
    "    val2_idx = range(train_len + val1_len, train_len + val1_len + val2_len)\n",
    "    kalman_idx = range(train_len + val1_len + val2_len, train_len + val1_len + val2_len + kalman_len)\n",
    "    test_idx = range(train_len + val1_len + val2_len + kalman_len, total_len)\n",
    "\n",
    "    return (\n",
    "        X.iloc[train_idx], X.iloc[val1_idx], X.iloc[val2_idx], X.iloc[kalman_idx], X.iloc[test_idx],\n",
    "        y.iloc[train_idx], y.iloc[val1_idx], y.iloc[val2_idx], y.iloc[kalman_idx], y.iloc[test_idx]\n",
    "    )\n",
    "\n",
    "\n",
    "def optimize_model_hyperparameters(model_fn, param_grid, X_train, y_train, validation_data, n_jobs=1):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter optimization using GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        model_fn: A callable that returns an instance of the model.\n",
    "        param_grid: Dictionary of hyperparameters to search.\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        validation_data: Tuple (X_val, y_val) for validation.\n",
    "        n_jobs: Number of parallel jobs for GridSearchCV.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The best model after GridSearchCV.\n",
    "        best_params: The best parameters from the search.\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics including Accuracy, Precision, Recall, F1, and AUC.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_pred_proba (array-like, optional): Predicted probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    if y_pred_proba is not None:\n",
    "        metrics[\"AUC\"] = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def preprocess_data_with_advanced_features(data_frame, target_column, lag_steps=None, rolling_window=10):\n",
    "    \"\"\"\n",
    "    Preprocess data for time series modeling with advanced feature engineering.\n",
    "    Ensures no data leakage by strictly using past and current data for feature generation.\n",
    "\n",
    "    Args:\n",
    "        data_frame (str): Variable name of loaded pandas data frame.\n",
    "        target_column (str): Target column name.\n",
    "        lag_steps (list): List of lag steps for feature engineering.\n",
    "        rolling_window (int): Window size for rolling features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature DataFrame (X) and target series (y).\n",
    "    \"\"\"\n",
    "    # Load data and parse dates\n",
    "    data = data_frame\n",
    "    data.index = pd.to_datetime(data.index, errors='coerce')  # Ensure index is datetime\n",
    "    assert data.index.is_monotonic_increasing, \"Dataset is not sorted by time.\"\n",
    "\n",
    "    # Fill missing values in the target column\n",
    "    data[target_column] = data[target_column].interpolate(method='linear').bfill()\n",
    "\n",
    "    # Initialize feature storage\n",
    "    features = []\n",
    "    indices = []\n",
    "\n",
    "    for end_idx in range(rolling_window, len(data)):\n",
    "        # Define the current window\n",
    "        window = data.iloc[end_idx - rolling_window:end_idx]\n",
    "\n",
    "        # Compute features for the current timestamp\n",
    "        current_features = {}\n",
    "\n",
    "        # Rolling statistics\n",
    "        signal_cols = [col for col in data.columns if col not in ['patient', 'newtest', 'target', 'event1', 'event2', 'event3', 'event4', 'sleepstage']]  # sleepstage excluded as categorical variable\n",
    "        for col in signal_cols:\n",
    "            current_features[f'{col}_roll_mean'] = window[col].mean()\n",
    "            current_features[f'{col}_roll_std'] = window[col].std()\n",
    "\n",
    "        # Lagged features\n",
    "        if lag_steps:\n",
    "            for lag in lag_steps:\n",
    "                if end_idx - lag >= 0:\n",
    "                    current_features[f'{target_column}_lag{lag}'] = data[target_column].iloc[end_idx - lag]\n",
    "\n",
    "        # Fourier Transform Features\n",
    "        for col in signal_cols:\n",
    "            fourier_transform = np.abs(np.fft.fft(window[col].fillna(0)))\n",
    "            current_features[f'{col}_fft_max'] = np.max(fourier_transform)\n",
    "            current_features[f'{col}_fft_mean'] = np.mean(fourier_transform)\n",
    "\n",
    "        # Wavelet Transform Features\n",
    "        for col in signal_cols:\n",
    "            coeffs = pywt.wavedec(window[col].fillna(0), 'db1', level=3)\n",
    "            current_features[f'{col}_wavelet_approx'] = coeffs[0].mean()\n",
    "            current_features[f'{col}_wavelet_detail1'] = coeffs[1].mean()\n",
    "            current_features[f'{col}_wavelet_detail2'] = coeffs[2].mean()\n",
    "\n",
    "        # Add features and corresponding index\n",
    "        features.append(current_features)\n",
    "        indices.append(data.index[end_idx])\n",
    "\n",
    "    # Convert features to DataFrame\n",
    "    feature_df = pd.DataFrame(features, index=indices)\n",
    "\n",
    "    # Align target values\n",
    "    y = data.loc[feature_df.index, target_column]\n",
    "\n",
    "    return feature_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load and Preprocess Data\n",
    "# -----------------\n",
    "\n",
    "def load_and_preprocess_data(dataframe):\n",
    "    # Load and preprocess data with advanced features\n",
    "    X, y = preprocess_data_with_advanced_features(\n",
    "        data_frame=dataframe,\n",
    "        target_column='target',\n",
    "        lag_steps=[1, 2, 3],\n",
    "        rolling_window=10\n",
    "    )\n",
    "\n",
    "    # Perform five-way split\n",
    "    X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test = five_way_split(\n",
    "        X, y, train_size=0.5, val1_size=0.15, val2_size=0.05, kalman_size=0.1, test_size=0.2\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Baselines\n",
    "# -----------------\n",
    "\n",
    "def baselines(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    # T-1 Baseline\n",
    "    y_t1_baseline = X_test[\"target_lag1\"].astype(int)\n",
    "    t1_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_t1_baseline),\n",
    "        \"Precision\": precision_score(y_test, y_t1_baseline, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_t1_baseline, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, y_t1_baseline, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, y_t1_baseline)\n",
    "    }\n",
    "    print(\"T-1 Baseline Metrics:\", t1_metrics)\n",
    "\n",
    "    # Random Classifier Baseline\n",
    "    def random_classifier(y_true, seed=42):\n",
    "        random.seed(seed)\n",
    "        return pd.Series([random.choice([0, 1]) for _ in range(len(y_true))], index=y_true.index)\n",
    "\n",
    "    y_random = random_classifier(y_test)\n",
    "    random_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_random),\n",
    "        \"Precision\": precision_score(y_test, y_random, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_random, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, y_random, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, y_random)\n",
    "    }\n",
    "    print(\"Random Classifier Metrics:\", random_metrics)\n",
    "\n",
    "    # Rolling Naive Bayes Baseline\n",
    "    def rolling_naive_bayes(train_series, test_series, window_size):\n",
    "        predictions = []\n",
    "        rolling_buffer = train_series.tail(window_size)\n",
    "\n",
    "        for test_point in test_series:\n",
    "            # Fit Naive Bayes on the rolling buffer\n",
    "            X_train = np.arange(len(rolling_buffer)).reshape(-1, 1)  # Sequential indices as features\n",
    "            y_train = rolling_buffer.values  # Targets\n",
    "\n",
    "            model = GaussianNB()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict the test point\n",
    "            X_test = np.array([[len(rolling_buffer)]]).reshape(-1, 1)\n",
    "            prediction = model.predict(X_test)\n",
    "            predictions.append(prediction[0])\n",
    "\n",
    "            # Update rolling buffer\n",
    "            rolling_buffer = pd.concat([rolling_buffer, pd.Series([test_point])], ignore_index=True)\n",
    "            if len(rolling_buffer) > window_size:\n",
    "                rolling_buffer = rolling_buffer.iloc[1:]\n",
    "\n",
    "        return pd.Series(predictions, index=test_series.index)\n",
    "\n",
    "    y_rolling_nb = rolling_naive_bayes(pd.concat([y_train, y_val1, y_val2]), y_test, WINDOW_SIZE)\n",
    "    rolling_nb_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_rolling_nb),\n",
    "        \"Precision\": precision_score(y_test, y_rolling_nb, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_rolling_nb, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, y_rolling_nb, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, y_rolling_nb)\n",
    "    }\n",
    "    print(\"Rolling Naive Bayes Metrics:\", rolling_nb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Base Models\n",
    "# -----------------\n",
    "\n",
    "# Logistic Regression\n",
    "def logistic_regression(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    log_reg_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logisticregression', LogisticRegression(class_weight='balanced'))  # Handle class imbalance\n",
    "    ])\n",
    "    log_reg_grid = GridSearchCV(log_reg_pipeline, LASSO_PARAM_GRID, cv=5, scoring='roc_auc')\n",
    "    log_reg_grid.fit(X_train, y_train)\n",
    "    log_reg_model = log_reg_grid.best_estimator_\n",
    "\n",
    "    log_reg_preds = log_reg_model.predict(X_test)\n",
    "    log_reg_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, log_reg_preds),\n",
    "        \"Precision\": precision_score(y_test, log_reg_preds, zero_division=0),  # Avoid warning\n",
    "        \"Recall\": recall_score(y_test, log_reg_preds, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, log_reg_preds, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, log_reg_model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    print(\"Logistic Regression Metrics:\", log_reg_metrics)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "def random_forest(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('randomforest', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced'))  # Handle class imbalance\n",
    "    ])\n",
    "    rf_param_grid = {\n",
    "        \"randomforest__n_estimators\": [50, 100, 200],  # Prefixed by 'randomforest__'\n",
    "        \"randomforest__max_depth\": [None, 10, 20]\n",
    "    }\n",
    "    rf_grid = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='roc_auc')\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    rf_model = rf_grid.best_estimator_\n",
    "\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, rf_preds),\n",
    "        \"Precision\": precision_score(y_test, rf_preds),\n",
    "        \"Recall\": recall_score(y_test, rf_preds),\n",
    "        \"F1\": f1_score(y_test, rf_preds),\n",
    "        \"AUC\": roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    print(\"Random Forest Metrics:\", rf_metrics)\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "def xgboost(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    # Define the pipeline\n",
    "    xgb_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "    # Prefix all hyperparameters for 'xgb' step with 'xgb__'\n",
    "    xgb_param_grid = {\n",
    "        \"xgb__n_estimators\": [50, 100, 200],\n",
    "        \"xgb__max_depth\": [3, 5, 7],\n",
    "        \"xgb__learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"xgb__subsample\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # Perform GridSearchCV\n",
    "    xgb_grid = GridSearchCV(xgb_pipeline, xgb_param_grid, cv=5, scoring='roc_auc')\n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "    xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "    # Predictions and metrics\n",
    "    xgb_preds = xgb_model.predict(X_test)\n",
    "    xgb_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, xgb_preds),\n",
    "        \"Precision\": precision_score(y_test, xgb_preds, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, xgb_preds, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, xgb_preds, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    print(\"XGBoost Metrics:\", xgb_metrics)\n",
    "\n",
    "\n",
    "# Lasso (Base)\n",
    "def lasso_base(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    lasso_base_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scaling for consistent input\n",
    "        ('lasso', LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear'))  # L1 Regularization\n",
    "    ])\n",
    "    lasso_base_param_grid = {\"lasso__C\": np.logspace(-3, 2, 10)}  # Regularization strength\n",
    "\n",
    "    lasso_base_grid = GridSearchCV(lasso_base_pipeline, lasso_base_param_grid, cv=5, scoring='roc_auc')\n",
    "    lasso_base_grid.fit(X_train, y_train)\n",
    "    lasso_base_model = lasso_base_grid.best_estimator_  # Capture the best Lasso model\n",
    "\n",
    "    # Predictions and Metrics\n",
    "    lasso_base_preds_proba = lasso_base_model.predict_proba(X_test)[:, 1]\n",
    "    lasso_base_preds = (lasso_base_preds_proba > 0.5).astype(int)\n",
    "\n",
    "    lasso_base_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, lasso_base_preds),\n",
    "        \"Precision\": precision_score(y_test, lasso_base_preds, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, lasso_base_preds, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, lasso_base_preds, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, lasso_base_preds_proba)\n",
    "    }\n",
    "    print(\"Lasso (Base) Metrics:\", lasso_base_metrics)\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "class BinaryNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BinaryNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def train_nn_with_hyperparams(X_train, y_train, X_val1, y_val1, param_grid):\n",
    "    \"\"\"Grid search for PyTorch NN.\"\"\"\n",
    "    best_params = None\n",
    "    best_auc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        hidden_size, lr, num_epochs = params\n",
    "        model = BinaryNN(input_size=X_train.shape[1], hidden_size=hidden_size)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        X_val1_tensor = torch.tensor(X_val1.values, dtype=torch.float32)\n",
    "        y_val1_tensor = torch.tensor(y_val1.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val1_tensor).flatten().numpy()\n",
    "        auc = roc_auc_score(y_val1, val_outputs)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, {\"AUC\": best_auc, \"Best Params\": best_params}\n",
    "\n",
    "def neural_network(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    nn_model, nn_metrics = train_nn_with_hyperparams(X_train, y_train, X_val1, y_val1, NN_PARAM_GRID)\n",
    "\n",
    "    # Predictions and Metrics for Test Set\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        nn_outputs = nn_model(X_test_tensor).flatten().numpy()\n",
    "    nn_preds = (nn_outputs > 0.5).astype(int)\n",
    "\n",
    "    nn_test_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, nn_preds),\n",
    "        \"Precision\": precision_score(y_test, nn_preds, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, nn_preds, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, nn_preds, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, nn_outputs)\n",
    "    }\n",
    "    print(\"Neural Network Test Metrics:\", nn_test_metrics)\n",
    "\n",
    "\n",
    "# LSTM Model for Classification\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.sigmoid(self.fc(hidden[-1]))\n",
    "        return x\n",
    "\n",
    "def train_lstm_with_hyperparams(X_train, y_train, X_val1, y_val1, param_grid):\n",
    "    \"\"\"Grid search for LSTM.\"\"\"\n",
    "    best_params = None\n",
    "    best_auc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        hidden_size, num_layers, lr, num_epochs = params\n",
    "        model = LSTMClassifier(input_size=X_train.shape[1], hidden_size=hidden_size, num_layers=num_layers)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train_seq = torch.tensor(X_train.values.reshape(-1, 1, X_train.shape[1]), dtype=torch.float32)\n",
    "        y_train_seq = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        X_val1_seq = torch.tensor(X_val1.values.reshape(-1, 1, X_val1.shape[1]), dtype=torch.float32)\n",
    "        y_val1_seq = torch.tensor(y_val1.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_seq)\n",
    "            loss = criterion(outputs, y_train_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val1_seq).flatten().numpy()\n",
    "        auc = roc_auc_score(y_val1, val_outputs)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, {\"AUC\": best_auc, \"Best Params\": best_params}\n",
    "\n",
    "def lstm(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    lstm_model, lstm_metrics = train_lstm_with_hyperparams(X_train, y_train, X_val1, y_val1, LSTM_PARAM_GRID)\n",
    "\n",
    "    # Predictions and Metrics for Test Set\n",
    "    X_test_seq = torch.tensor(X_test.values.reshape(-1, 1, X_test.shape[1]), dtype=torch.float32)\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_outputs = lstm_model(X_test_seq).flatten().numpy()\n",
    "    lstm_preds = (lstm_outputs > 0.5).astype(int)\n",
    "\n",
    "    lstm_test_metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, lstm_preds),\n",
    "        \"Precision\": precision_score(y_test, lstm_preds, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, lstm_preds, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, lstm_preds, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, lstm_outputs)\n",
    "    }\n",
    "    print(\"LSTM Test Metrics:\", lstm_test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with data batch1 127 patients -> 64 ideal patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "master_df = pd.read_stata('../data/stmary/processed-data/combined-patient-data-1_00.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group master dataframe by 'patient' and 'newtest' pairs (i.e. by each unique patient data)\n",
    "# Access or initialize each dataframe like: group_dict[('pid100100', 0)]\n",
    "group_dict = {\n",
    "    (val1, val2): data\n",
    "    for (val1, val2), data in master_df.groupby(['patient', 'newtest'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframes\n",
    "for group_key, subset_df in group_dict.items():\n",
    "    subset_df['target'] = subset_df[['event1', 'event2', 'event3', 'event4']].apply(lambda x: 1 if 'Hypopnea' in x.values or 'Apnea Obstructive' in x.values or 'Apnea Central' in x.values or 'Apnea Mixed' in x.values else 0, axis=1)\n",
    "\n",
    "    cols = list(subset_df.columns)\n",
    "    cols.remove('target')\n",
    "    cols.insert(3, 'target')\n",
    "    subset_df = subset_df[cols]\n",
    "\n",
    "    subset_df.set_index('timess', inplace=True)\n",
    "    \n",
    "    group_dict[group_key] = subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ideal_group_dict from group_dict using ideal_patients\n",
    "ideal_patients = pd.read_csv('../data/stmary/raw-data/ideal_patients_1.csv')\n",
    "ideal_patients = list(ideal_patients.itertuples(index=False, name=None))\n",
    "\n",
    "ideal_group_dict = {\n",
    "    (val1, val2): group_dict[(val1, val2)]\n",
    "    for (val1, val2) in ideal_patients\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pid100816', 0),\n",
       " ('pid102234', 0),\n",
       " ('pid103968', 0),\n",
       " ('pid109461', 0),\n",
       " ('pid166861', 0),\n",
       " ('pid179590', 0),\n",
       " ('pid183019', 0),\n",
       " ('pid199445', 0),\n",
       " ('pid208588', 0),\n",
       " ('pid215758', 0),\n",
       " ('pid219965', 0),\n",
       " ('pid224699', 0),\n",
       " ('pid234620', 0),\n",
       " ('pid261476', 0),\n",
       " ('pid283707', 0),\n",
       " ('pid286719', 0),\n",
       " ('pid286935', 0),\n",
       " ('pid293192', 0),\n",
       " ('pid300006', 0),\n",
       " ('pid310946', 0),\n",
       " ('pid311262', 0),\n",
       " ('pid317963', 0),\n",
       " ('pid327156', 0),\n",
       " ('pid334988', 0),\n",
       " ('pid350887', 0),\n",
       " ('pid367205', 0),\n",
       " ('pid369605', 0),\n",
       " ('pid391110', 0),\n",
       " ('pid391482', 0),\n",
       " ('pid392875', 0),\n",
       " ('pid393748', 0),\n",
       " ('pid398076', 0),\n",
       " ('pid401870', 0),\n",
       " ('pid403691', 0),\n",
       " ('pid408642', 0),\n",
       " ('pid417827', 0),\n",
       " ('pid431521', 0),\n",
       " ('pid442085', 0),\n",
       " ('pid450941', 0),\n",
       " ('pid456686', 0),\n",
       " ('pid464160', 0),\n",
       " ('pid480595', 0),\n",
       " ('pid481987', 0),\n",
       " ('pid527128', 0),\n",
       " ('pid545303', 0),\n",
       " ('pid555964', 0),\n",
       " ('pid561591', 0),\n",
       " ('pid564533', 0),\n",
       " ('pid569689', 0),\n",
       " ('pid577268', 0),\n",
       " ('pid583200', 0),\n",
       " ('pid592809', 0),\n",
       " ('pid595815', 0),\n",
       " ('pid624071', 0),\n",
       " ('pid627078', 0),\n",
       " ('pid631889', 0),\n",
       " ('pid637827', 0),\n",
       " ('pid639355', 0),\n",
       " ('pid644133', 0),\n",
       " ('pid649364', 0),\n",
       " ('pid650939', 0),\n",
       " ('pid659360', 0),\n",
       " ('pid674621', 0),\n",
       " ('pid682732', 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with ideal pid100816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_100816 = load_and_preprocess_data(ideal_group_dict[('pid100816', 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaN from Xs and corresponding rows from ys\n",
    "def drop_nan_from_Xy(X_train, X_val1, X_val2, X_kalman, X_test, y_train, y_val1, y_val2, y_kalman, y_test):\n",
    "    # Drop rows with NaN from X\n",
    "    X_train_cleaned = X_train.dropna()\n",
    "    X_val1_cleaned = X_val1.dropna()\n",
    "    X_val2_cleaned = X_val2.dropna()\n",
    "    X_kalman_cleaned = X_kalman.dropna()\n",
    "    X_test_cleaned = X_test.dropna()\n",
    "    \n",
    "    # Drop corresponding rows from y\n",
    "    y_train_cleaned = y_train[X_train_cleaned.index]\n",
    "    y_val1_cleaned = y_val1[X_val1_cleaned.index]\n",
    "    y_val2_cleaned = y_val2[X_val2_cleaned.index]\n",
    "    y_kalman_cleaned = y_kalman[X_kalman_cleaned.index]\n",
    "    y_test_cleaned = y_test[X_test_cleaned.index]\n",
    "    \n",
    "    return (\n",
    "        X_train_cleaned, X_val1_cleaned, X_val2_cleaned, X_kalman_cleaned, X_test_cleaned,\n",
    "        y_train_cleaned, y_val1_cleaned, y_val2_cleaned, y_kalman_cleaned, y_test_cleaned\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_100816_cleaned = drop_nan_from_Xy(*Xy_100816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics: {'Accuracy': 0.9994317692638829, 'Precision': 0.982089552238806, 'Recall': 0.9932075471698113, 'F1': 0.9876172607879925, 'AUC': np.float64(0.9977978355913889)}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(*Xy_100816_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics: {'Accuracy': 0.9995695221696083, 'Precision': 0.9961832061068703, 'Recall': 0.9849056603773585, 'F1': 0.9905123339658444, 'AUC': np.float64(0.9992320372371374)}\n"
     ]
    }
   ],
   "source": [
    "random_forest(*Xy_100816_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Metrics: {'Accuracy': 0.9998278088678433, 'Precision': 0.9962264150943396, 'Recall': 0.9962264150943396, 'F1': 0.9962264150943396, 'AUC': np.float64(0.9986167600365722)}\n"
     ]
    }
   ],
   "source": [
    "xgboost(*Xy_100816_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (Base) Metrics: {'Accuracy': 0.9998278088678433, 'Precision': 0.9962264150943396, 'Recall': 0.9962264150943396, 'F1': 0.9962264150943396, 'AUC': np.float64(0.9981001080541934)}\n"
     ]
    }
   ],
   "source": [
    "lasso_base(*Xy_100816_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Test Metrics: {'Accuracy': 0.35266465777012485, 'Precision': 0.025783844564734187, 'Recall': 0.7441509433962264, 'F1': 0.04984077238032654, 'AUC': np.float64(0.5848721336547253)}\n"
     ]
    }
   ],
   "source": [
    "neural_network(*Xy_100816_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test Metrics: {'Accuracy': 0.977184674989238, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.48701045632117035)}\n"
     ]
    }
   ],
   "source": [
    "lstm(*Xy_100816_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running all tests for pid 100816 (Base Model, Ensemble, Kalman Filter Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Xy_100816_cleaned[0]\n",
    "X_val1 = Xy_100816_cleaned[1]\n",
    "X_val2 = Xy_100816_cleaned[2]\n",
    "X_kalman = Xy_100816_cleaned[3]\n",
    "X_test = Xy_100816_cleaned[4]\n",
    "y_train = Xy_100816_cleaned[5]\n",
    "y_val1 = Xy_100816_cleaned[6]\n",
    "y_val2 = Xy_100816_cleaned[7]\n",
    "y_kalman = Xy_100816_cleaned[8]\n",
    "y_test = Xy_100816_cleaned[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-1 Baseline Metrics: {'Accuracy': 0.9998278088678433, 'Precision': 0.9962264150943396, 'Recall': 0.9962264150943396, 'F1': 0.9962264150943396, 'AUC': np.float64(0.9980691546837337)}\n",
      "Random Classifier Metrics: {'Accuracy': 0.49773568661213946, 'Precision': 0.022597901378506275, 'Recall': 0.49735849056603776, 'F1': 0.043231541312690656, 'AUC': np.float64(0.49755149197905413)}\n",
      "Rolling Naive Bayes Metrics: {'Accuracy': 0.99948342660353, 'Precision': 0.9886792452830189, 'Recall': 0.9886792452830189, 'F1': 0.9886792452830189, 'AUC': np.float64(0.9942074640512011)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Baselines\n",
    "# -----------------\n",
    "\n",
    "# T-1 Baseline\n",
    "y_t1_baseline = X_test[\"target_lag1\"].astype(int)\n",
    "t1_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_t1_baseline),\n",
    "    \"Precision\": precision_score(y_test, y_t1_baseline, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, y_t1_baseline, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, y_t1_baseline, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, y_t1_baseline)\n",
    "}\n",
    "print(\"T-1 Baseline Metrics:\", t1_metrics)\n",
    "\n",
    "# Random Classifier Baseline\n",
    "def random_classifier(y_true, seed=42):\n",
    "    random.seed(seed)\n",
    "    return pd.Series([random.choice([0, 1]) for _ in range(len(y_true))], index=y_true.index)\n",
    "\n",
    "y_random = random_classifier(y_test)\n",
    "random_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_random),\n",
    "    \"Precision\": precision_score(y_test, y_random, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, y_random, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, y_random, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, y_random)\n",
    "}\n",
    "print(\"Random Classifier Metrics:\", random_metrics)\n",
    "\n",
    "# Rolling Naive Bayes Baseline\n",
    "def rolling_naive_bayes(train_series, test_series, window_size):\n",
    "    predictions = []\n",
    "    rolling_buffer = train_series.tail(window_size)\n",
    "\n",
    "    for test_point in test_series:\n",
    "        # Fit Naive Bayes on the rolling buffer\n",
    "        X_train = np.arange(len(rolling_buffer)).reshape(-1, 1)  # Sequential indices as features\n",
    "        y_train = rolling_buffer.values  # Targets\n",
    "\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the test point\n",
    "        X_test = np.array([[len(rolling_buffer)]]).reshape(-1, 1)\n",
    "        prediction = model.predict(X_test)\n",
    "        predictions.append(prediction[0])\n",
    "\n",
    "        # Update rolling buffer\n",
    "        rolling_buffer = pd.concat([rolling_buffer, pd.Series([test_point])], ignore_index=True)\n",
    "        if len(rolling_buffer) > window_size:\n",
    "            rolling_buffer = rolling_buffer.iloc[1:]\n",
    "\n",
    "    return pd.Series(predictions, index=test_series.index)\n",
    "\n",
    "\n",
    "y_rolling_nb = rolling_naive_bayes(pd.concat([y_train, y_val1, y_val2]), y_test, WINDOW_SIZE)\n",
    "rolling_nb_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_rolling_nb),\n",
    "    \"Precision\": precision_score(y_test, y_rolling_nb, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, y_rolling_nb, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, y_rolling_nb, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, y_rolling_nb)\n",
    "}\n",
    "print(\"Rolling Naive Bayes Metrics:\", rolling_nb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics: {'Accuracy': 0.9994317692638829, 'Precision': 0.982089552238806, 'Recall': 0.9932075471698113, 'F1': 0.9876172607879925, 'AUC': np.float64(0.9977978355913889)}\n",
      "Random Forest Metrics: {'Accuracy': 0.9995695221696083, 'Precision': 0.9961832061068703, 'Recall': 0.9849056603773585, 'F1': 0.9905123339658444, 'AUC': np.float64(0.9992320372371374)}\n",
      "XGBoost Metrics: {'Accuracy': 0.9998278088678433, 'Precision': 0.9962264150943396, 'Recall': 0.9962264150943396, 'F1': 0.9962264150943396, 'AUC': np.float64(0.9986167600365722)}\n",
      "Lasso (Base) Metrics: {'Accuracy': 0.9998278088678433, 'Precision': 0.9962264150943396, 'Recall': 0.9962264150943396, 'F1': 0.9962264150943396, 'AUC': np.float64(0.9980983525891447)}\n",
      "Neural Network Test Metrics: {'Accuracy': 0.9770124838570814, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.5492772338126506)}\n",
      "LSTM Test Metrics: {'Accuracy': 0.977184674989238, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.485963906574682)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Base Models\n",
    "# -----------------\n",
    "# -----------------\n",
    "# Logistic Regression\n",
    "# -----------------\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression(class_weight='balanced'))  # Handle class imbalance\n",
    "])\n",
    "log_reg_grid = GridSearchCV(log_reg_pipeline, LASSO_PARAM_GRID, cv=5, scoring='roc_auc')\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "log_reg_model = log_reg_grid.best_estimator_\n",
    "\n",
    "log_reg_preds = log_reg_model.predict(X_test)\n",
    "log_reg_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, log_reg_preds),\n",
    "    \"Precision\": precision_score(y_test, log_reg_preds, zero_division=0),  # Avoid warning\n",
    "    \"Recall\": recall_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, log_reg_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Logistic Regression Metrics:\", log_reg_metrics)\n",
    "\n",
    "# -----------------\n",
    "# Random Forest\n",
    "# -----------------\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('randomforest', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced'))  # Handle class imbalance\n",
    "])\n",
    "rf_param_grid = {\n",
    "    \"randomforest__n_estimators\": [50, 100, 200],  # Prefixed by 'randomforest__'\n",
    "    \"randomforest__max_depth\": [None, 10, 20]\n",
    "}\n",
    "rf_grid = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='roc_auc')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_model = rf_grid.best_estimator_\n",
    "\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, rf_preds),\n",
    "    \"Precision\": precision_score(y_test, rf_preds),\n",
    "    \"Recall\": recall_score(y_test, rf_preds),\n",
    "    \"F1\": f1_score(y_test, rf_preds),\n",
    "    \"AUC\": roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Random Forest Metrics:\", rf_metrics)\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "# Define the pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Prefix all hyperparameters for 'xgb' step with 'xgb__'\n",
    "xgb_param_grid = {\n",
    "    \"xgb__n_estimators\": [50, 100, 200],\n",
    "    \"xgb__max_depth\": [3, 5, 7],\n",
    "    \"xgb__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"xgb__subsample\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "xgb_grid = GridSearchCV(xgb_pipeline, xgb_param_grid, cv=5, scoring='roc_auc')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "# Predictions and metrics\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, xgb_preds),\n",
    "    \"Precision\": precision_score(y_test, xgb_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, xgb_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, xgb_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"XGBoost Metrics:\", xgb_metrics)\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Lasso (Base)\n",
    "# -----------------\n",
    "lasso_base_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling for consistent input\n",
    "    ('lasso', LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear'))  # L1 Regularization\n",
    "])\n",
    "lasso_base_param_grid = {\"lasso__C\": np.logspace(-3, 2, 10)}  # Regularization strength\n",
    "\n",
    "lasso_base_grid = GridSearchCV(lasso_base_pipeline, lasso_base_param_grid, cv=5, scoring='roc_auc')\n",
    "lasso_base_grid.fit(X_train, y_train)\n",
    "lasso_base_model = lasso_base_grid.best_estimator_  # Capture the best Lasso model\n",
    "\n",
    "# Predictions and Metrics\n",
    "lasso_base_preds_proba = lasso_base_model.predict_proba(X_test)[:, 1]\n",
    "lasso_base_preds = (lasso_base_preds_proba > 0.5).astype(int)\n",
    "\n",
    "lasso_base_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, lasso_base_preds),\n",
    "    \"Precision\": precision_score(y_test, lasso_base_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, lasso_base_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, lasso_base_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, lasso_base_preds_proba)\n",
    "}\n",
    "print(\"Lasso (Base) Metrics:\", lasso_base_metrics)\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Neural Network Models\n",
    "# -----------------\n",
    "class BinaryNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BinaryNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def train_nn_with_hyperparams(X_train, y_train, X_val1, y_val1, param_grid):\n",
    "    \"\"\"Grid search for PyTorch NN.\"\"\"\n",
    "    best_params = None\n",
    "    best_auc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        hidden_size, lr, num_epochs = params\n",
    "        model = BinaryNN(input_size=X_train.shape[1], hidden_size=hidden_size)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        X_val1_tensor = torch.tensor(X_val1.values, dtype=torch.float32)\n",
    "        y_val1_tensor = torch.tensor(y_val1.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val1_tensor).flatten().numpy()\n",
    "        auc = roc_auc_score(y_val1, val_outputs)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, {\"AUC\": best_auc, \"Best Params\": best_params}\n",
    "\n",
    "# Train and evaluate NN\n",
    "nn_model, nn_metrics = train_nn_with_hyperparams(X_train, y_train, X_val1, y_val1, NN_PARAM_GRID)\n",
    "\n",
    "# Predictions and Metrics for Test Set\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_outputs = nn_model(X_test_tensor).flatten().numpy()\n",
    "nn_preds = (nn_outputs > 0.5).astype(int)\n",
    "\n",
    "nn_test_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, nn_preds),\n",
    "    \"Precision\": precision_score(y_test, nn_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, nn_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, nn_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, nn_outputs)\n",
    "}\n",
    "print(\"Neural Network Test Metrics:\", nn_test_metrics)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model for Classification\n",
    "# -----------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.sigmoid(self.fc(hidden[-1]))\n",
    "        return x\n",
    "\n",
    "def train_lstm_with_hyperparams(X_train, y_train, X_val1, y_val1, param_grid):\n",
    "    \"\"\"Grid search for LSTM.\"\"\"\n",
    "    best_params = None\n",
    "    best_auc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        hidden_size, num_layers, lr, num_epochs = params\n",
    "        model = LSTMClassifier(input_size=X_train.shape[1], hidden_size=hidden_size, num_layers=num_layers)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train_seq = torch.tensor(X_train.values.reshape(-1, 1, X_train.shape[1]), dtype=torch.float32)\n",
    "        y_train_seq = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        X_val1_seq = torch.tensor(X_val1.values.reshape(-1, 1, X_val1.shape[1]), dtype=torch.float32)\n",
    "        y_val1_seq = torch.tensor(y_val1.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_seq)\n",
    "            loss = criterion(outputs, y_train_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val1_seq).flatten().numpy()\n",
    "        auc = roc_auc_score(y_val1, val_outputs)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, {\"AUC\": best_auc, \"Best Params\": best_params}\n",
    "\n",
    "# Train and evaluate LSTM\n",
    "lstm_model, lstm_metrics = train_lstm_with_hyperparams(X_train, y_train, X_val1, y_val1, LSTM_PARAM_GRID)\n",
    "\n",
    "# Predictions and Metrics for Test Set\n",
    "X_test_seq = torch.tensor(X_test.values.reshape(-1, 1, X_test.shape[1]), dtype=torch.float32)\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    lstm_outputs = lstm_model(X_test_seq).flatten().numpy()\n",
    "lstm_preds = (lstm_outputs > 0.5).astype(int)\n",
    "\n",
    "lstm_test_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, lstm_preds),\n",
    "    \"Precision\": precision_score(y_test, lstm_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, lstm_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, lstm_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, lstm_outputs)\n",
    "}\n",
    "print(\"LSTM Test Metrics:\", lstm_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Ensemble Metrics: {'Accuracy': 0.9997589324149806, 'Precision': 0.9962149886449659, 'Recall': 0.9932075471698113, 'F1': 0.9947089947089947, 'AUC': np.float64(0.9983610339955116)}\n",
      "Stacking Ensemble Metrics: {'Accuracy': 0.9998278088678433, 'Precision': 0.9962264150943396, 'Recall': 0.9962264150943396, 'F1': 0.9962264150943396, 'AUC': np.float64(0.9991009359155516)}\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Lasso Ensemble Metrics (Optimized): {'Accuracy': 0.34743004735256133, 'Precision': 0.01706061006206259, 'Recall': 0.48754716981132074, 'F1': 0.032967593773921916, 'AUC': np.float64(0.4583647311112958)}\n",
      "Neural Network Ensemble Metrics (Optimized): {'Accuracy': 0.9893930262591476, 'Precision': 0.9971949509116409, 'Recall': 0.5366037735849056, 'F1': 0.6977428851815506, 'AUC': np.float64(0.998689731526889)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Align Features Across Datasets\n",
    "# -----------------\n",
    "common_features = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train[common_features]\n",
    "X_val1 = X_val1[common_features]\n",
    "X_test = X_test[common_features]\n",
    "\n",
    "# -----------------\n",
    "# Preprocessing Pipeline\n",
    "# -----------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# -----------------\n",
    "# Base Model Predictions\n",
    "# -----------------\n",
    "# Logistic Regression\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', log_reg_model)\n",
    "])\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "log_reg_preds_proba = log_reg_pipeline.predict_proba(X_test)[:, 1]\n",
    "log_reg_preds = (log_reg_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_preds_proba = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "rf_preds = (rf_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# XGBoost\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "xgb_preds_proba = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "xgb_preds = (xgb_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# Lasso\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', lasso_base_model)  # Ensure lasso_base_model is defined earlier\n",
    "])\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "lasso_preds_proba = lasso_pipeline.predict_proba(X_test)[:, 1]\n",
    "lasso_preds = (lasso_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# Neural Network (NN)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_preds_proba = nn_model(X_test_tensor).flatten().numpy()\n",
    "nn_preds = (nn_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# LSTM\n",
    "X_test_seq = torch.tensor(X_test.values.reshape(-1, 1, X_test.shape[1]), dtype=torch.float32)\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    lstm_preds_proba = lstm_model(X_test_seq).flatten().numpy()\n",
    "lstm_preds = (lstm_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# -----------------\n",
    "# Ensemble Predictions\n",
    "# -----------------\n",
    "ensemble_predictions_test = np.column_stack([\n",
    "    log_reg_preds_proba,\n",
    "    rf_preds_proba,\n",
    "    xgb_preds_proba,\n",
    "    lasso_preds_proba,  # Include Lasso predictions\n",
    "    nn_preds_proba,\n",
    "    lstm_preds_proba,\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Ensemble Predictions for Train, Validation, and Test\n",
    "# -----------------\n",
    "# Generate base model predictions for the training set\n",
    "log_reg_preds_train_proba = log_reg_pipeline.predict_proba(X_train)[:, 1]\n",
    "rf_preds_train_proba = rf_pipeline.predict_proba(X_train)[:, 1]\n",
    "xgb_preds_train_proba = xgb_pipeline.predict_proba(X_train)[:, 1]\n",
    "lasso_preds_train_proba = lasso_pipeline.predict_proba(X_train)[:, 1]\n",
    "with torch.no_grad():\n",
    "    nn_preds_train_proba = nn_model(torch.tensor(X_train.values, dtype=torch.float32)).flatten().numpy()\n",
    "    lstm_preds_train_proba = lstm_model(torch.tensor(X_train.values.reshape(-1, 1, X_train.shape[1]), dtype=torch.float32)).flatten().numpy()\n",
    "\n",
    "ensemble_predictions_train = np.column_stack([\n",
    "    log_reg_preds_train_proba,\n",
    "    rf_preds_train_proba,\n",
    "    xgb_preds_train_proba,\n",
    "    lasso_preds_train_proba,\n",
    "    nn_preds_train_proba,\n",
    "    lstm_preds_train_proba,\n",
    "])\n",
    "\n",
    "# Generate base model predictions for the validation set\n",
    "log_reg_preds_val_proba = log_reg_pipeline.predict_proba(X_val1)[:, 1]\n",
    "rf_preds_val_proba = rf_pipeline.predict_proba(X_val1)[:, 1]\n",
    "xgb_preds_val_proba = xgb_pipeline.predict_proba(X_val1)[:, 1]\n",
    "lasso_preds_val_proba = lasso_pipeline.predict_proba(X_val1)[:, 1]\n",
    "with torch.no_grad():\n",
    "    nn_preds_val_proba = nn_model(torch.tensor(X_val1.values, dtype=torch.float32)).flatten().numpy()\n",
    "    lstm_preds_val_proba = lstm_model(torch.tensor(X_val1.values.reshape(-1, 1, X_val1.shape[1]), dtype=torch.float32)).flatten().numpy()\n",
    "\n",
    "ensemble_predictions_val = np.column_stack([\n",
    "    log_reg_preds_val_proba,\n",
    "    rf_preds_val_proba,\n",
    "    xgb_preds_val_proba,\n",
    "    lasso_preds_val_proba,\n",
    "    nn_preds_val_proba,\n",
    "    lstm_preds_val_proba,\n",
    "])\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Weighted Average Ensemble\n",
    "# -----------------\n",
    "def optimize_weights_classification(predictions, y_true):\n",
    "    def loss_function(weights):\n",
    "        ensemble_probs = np.dot(predictions, weights)\n",
    "        return -roc_auc_score(y_true, ensemble_probs)  # Maximize AUC\n",
    "\n",
    "    initial_weights = np.ones(predictions.shape[1]) / predictions.shape[1]\n",
    "    constraints = {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1}\n",
    "    bounds = [(0, 1)] * predictions.shape[1]\n",
    "\n",
    "    result = minimize(loss_function, initial_weights, constraints=constraints, bounds=bounds)\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Weight optimization failed: \" + result.message)\n",
    "    return result.x\n",
    "\n",
    "# Optimize weights\n",
    "optimized_weights = optimize_weights_classification(ensemble_predictions_test, y_test)\n",
    "weighted_ensemble_probs = np.dot(ensemble_predictions_test, optimized_weights)\n",
    "weighted_ensemble_preds = (weighted_ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "weighted_ensemble_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, weighted_ensemble_preds),\n",
    "    \"Precision\": precision_score(y_test, weighted_ensemble_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, weighted_ensemble_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, weighted_ensemble_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, weighted_ensemble_probs)\n",
    "}\n",
    "print(\"Weighted Ensemble Metrics:\", weighted_ensemble_metrics)\n",
    "\n",
    "# -----------------\n",
    "# Stacking Ensemble\n",
    "# -----------------\n",
    "stacking_meta_model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "stacking_meta_model.fit(ensemble_predictions_test, y_test)\n",
    "\n",
    "stacking_probs = stacking_meta_model.predict_proba(ensemble_predictions_test)[:, 1]\n",
    "stacking_preds = (stacking_probs > 0.5).astype(int)\n",
    "\n",
    "stacking_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, stacking_preds),\n",
    "    \"Precision\": precision_score(y_test, stacking_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, stacking_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, stacking_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, stacking_probs)\n",
    "}\n",
    "print(\"Stacking Ensemble Metrics:\", stacking_metrics)\n",
    "\n",
    "# -----------------\n",
    "# Lasso Ensemble Meta-Model\n",
    "# -----------------\n",
    "lasso_ensemble_param_grid = {\"lasso__C\": np.logspace(-6, 2, 20)}  # Expanded parameter range\n",
    "lasso_ensemble_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Optimize hyperparameters for Lasso ensemble\n",
    "lasso_ensemble_model, best_lasso_ensemble_params = optimize_model_hyperparameters(\n",
    "    lambda: lasso_ensemble_pipeline,\n",
    "    lasso_ensemble_param_grid,\n",
    "    ensemble_predictions_train,  # Base model predictions as input\n",
    "    y_train,\n",
    "    validation_data=(ensemble_predictions_val, y_val1),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Fit the Lasso ensemble\n",
    "lasso_ensemble_model.fit(ensemble_predictions_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "lasso_ensemble_probs = lasso_ensemble_model.predict_proba(ensemble_predictions_test)[:, 1]\n",
    "lasso_ensemble_preds = (lasso_ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "lasso_ensemble_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, lasso_ensemble_preds),\n",
    "    \"Precision\": precision_score(y_test, lasso_ensemble_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, lasso_ensemble_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, lasso_ensemble_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, lasso_ensemble_probs)\n",
    "}\n",
    "print(\"Lasso Ensemble Metrics (Optimized):\", lasso_ensemble_metrics)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Neural Network Ensemble\n",
    "# -----------------\n",
    "\n",
    "nn_ensemble_param_grid = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100, 200]\n",
    "}\n",
    "\n",
    "best_nn_ensemble_model = None\n",
    "best_nn_ensemble_auc = 0\n",
    "best_nn_ensemble_params = None\n",
    "\n",
    "for params in product(*nn_ensemble_param_grid.values()):\n",
    "    hidden_size, lr, num_epochs = params\n",
    "    nn_ensemble_model = BinaryNN(input_size=ensemble_predictions_test.shape[1], hidden_size=hidden_size)\n",
    "    optimizer = optim.Adam(nn_ensemble_model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    X_train_tensor = torch.tensor(ensemble_predictions_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        nn_ensemble_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nn_ensemble_model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    nn_ensemble_model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = nn_ensemble_model(X_train_tensor).flatten().numpy()\n",
    "    auc = roc_auc_score(y_train, probs)\n",
    "\n",
    "    if auc > best_nn_ensemble_auc:\n",
    "        best_nn_ensemble_auc = auc\n",
    "        best_nn_ensemble_params = params\n",
    "        best_nn_ensemble_model = nn_ensemble_model\n",
    "\n",
    "# Evaluate best NN ensemble on the test set\n",
    "X_test_tensor = torch.tensor(ensemble_predictions_test, dtype=torch.float32)  # Use test set predictions\n",
    "best_nn_ensemble_model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_ensemble_probs = best_nn_ensemble_model(X_test_tensor).flatten().numpy()\n",
    "nn_ensemble_preds = (nn_ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "nn_ensemble_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, nn_ensemble_preds),\n",
    "    \"Precision\": precision_score(y_test, nn_ensemble_preds, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, nn_ensemble_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, nn_ensemble_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, nn_ensemble_probs)\n",
    "}\n",
    "print(\"Neural Network Ensemble Metrics (Optimized):\", nn_ensemble_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Kalman Filter Integration for Base Models\n",
    "# -----------------\n",
    "\n",
    "# Logistic Regression Integration with Kalman Filters\n",
    "log_reg_predictions_trimmed = log_reg_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for Logistic Regression\n",
    "cvkf_params_log_reg, cvkf_preds_log_reg = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    log_reg_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_log_reg = calculate_classification_metrics(y_test, (cvkf_preds_log_reg > 0.5).astype(int), cvkf_preds_log_reg)\n",
    "\n",
    "# FMKF for Logistic Regression\n",
    "fmkf_params_log_reg, fmkf_preds_log_reg = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    log_reg_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_log_reg = calculate_classification_metrics(y_test, (fmkf_preds_log_reg > 0.5).astype(int), fmkf_preds_log_reg)\n",
    "\n",
    "# Random Forest Integration with Kalman Filters\n",
    "rf_predictions_trimmed = rf_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for Random Forest\n",
    "cvkf_params_rf, cvkf_preds_rf = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    rf_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_rf = calculate_classification_metrics(y_test, (cvkf_preds_rf > 0.5).astype(int), cvkf_preds_rf)\n",
    "\n",
    "# FMKF for Random Forest\n",
    "fmkf_params_rf, fmkf_preds_rf = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    rf_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_rf = calculate_classification_metrics(y_test, (fmkf_preds_rf > 0.5).astype(int), fmkf_preds_rf)\n",
    "\n",
    "# XGBoost Integration with Kalman Filters\n",
    "xgb_predictions_trimmed = xgb_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for XGBoost\n",
    "cvkf_params_xgb, cvkf_preds_xgb = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    xgb_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_xgb = calculate_classification_metrics(y_test, (cvkf_preds_xgb > 0.5).astype(int), cvkf_preds_xgb)\n",
    "\n",
    "# FMKF for XGBoost\n",
    "fmkf_params_xgb, fmkf_preds_xgb = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    xgb_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_xgb = calculate_classification_metrics(y_test, (fmkf_preds_xgb > 0.5).astype(int), fmkf_preds_xgb)\n",
    "\n",
    "# -----------------\n",
    "# Stacking Ensemble Integration with Kalman Filters\n",
    "# -----------------\n",
    "stacking_predictions_trimmed = stacking_probs[:len(y_test)]  # Ensure alignment with y_test\n",
    "\n",
    "# CVKF for Stacking Ensemble\n",
    "cvkf_params_stack, cvkf_preds_stack = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    stacking_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_stack = calculate_classification_metrics(y_test, (cvkf_preds_stack > 0.5).astype(int), cvkf_preds_stack)\n",
    "\n",
    "# FMKF for Stacking Ensemble\n",
    "fmkf_params_stack, fmkf_preds_stack = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    stacking_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_stack = calculate_classification_metrics(y_test, (fmkf_preds_stack > 0.5).astype(int), fmkf_preds_stack)\n",
    "\n",
    "# -----------------\n",
    "# Weighted Ensemble Integration with Kalman Filters\n",
    "# -----------------\n",
    "weighted_predictions_trimmed = weighted_ensemble_probs[:len(y_test)]  # Trim predictions\n",
    "\n",
    "# CVKF for Weighted Ensemble\n",
    "cvkf_params_weight, cvkf_preds_weight = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    weighted_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_weight = calculate_classification_metrics(y_test, (cvkf_preds_weight > 0.5).astype(int), cvkf_preds_weight)\n",
    "\n",
    "# FMKF for Weighted Ensemble\n",
    "fmkf_params_weight, fmkf_preds_weight = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    weighted_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_weight = calculate_classification_metrics(y_test, (fmkf_preds_weight > 0.5).astype(int), fmkf_preds_weight)\n",
    "\n",
    "# -----------------\n",
    "# Lasso Ensemble Integration with Kalman Filters\n",
    "# -----------------\n",
    "lasso_predictions_trimmed = lasso_ensemble_probs[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for Lasso Ensemble\n",
    "cvkf_params_lasso, cvkf_preds_lasso = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    lasso_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_lasso = calculate_classification_metrics(y_test, (cvkf_preds_lasso > 0.5).astype(int), cvkf_preds_lasso)\n",
    "\n",
    "# FMKF for Lasso Ensemble\n",
    "fmkf_params_lasso, fmkf_preds_lasso = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    lasso_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_lasso = calculate_classification_metrics(y_test, (fmkf_preds_lasso > 0.5).astype(int), fmkf_preds_lasso)\n",
    "\n",
    "# -----------------\n",
    "# Neural Network Integration with Kalman Filters\n",
    "# -----------------\n",
    "nn_predictions_trimmed = nn_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for Neural Network\n",
    "cvkf_params_nn, cvkf_preds_nn = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    nn_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_nn = calculate_classification_metrics(y_test, (cvkf_preds_nn > 0.5).astype(int), cvkf_preds_nn)\n",
    "\n",
    "# FMKF for Neural Network\n",
    "fmkf_params_nn, fmkf_preds_nn = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    nn_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_nn = calculate_classification_metrics(y_test, (fmkf_preds_nn > 0.5).astype(int), fmkf_preds_nn)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Integration with Kalman Filters\n",
    "# -----------------\n",
    "lstm_predictions_trimmed = lstm_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for LSTM\n",
    "cvkf_params_lstm, cvkf_preds_lstm = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    lstm_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_lstm = calculate_classification_metrics(y_test, (cvkf_preds_lstm > 0.5).astype(int), cvkf_preds_lstm)\n",
    "\n",
    "# FMKF for LSTM\n",
    "fmkf_params_lstm, fmkf_preds_lstm = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    lstm_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_lstm = calculate_classification_metrics(y_test, (fmkf_preds_lstm > 0.5).astype(int), fmkf_preds_lstm)\n",
    "\n",
    "# -----------------\n",
    "# Kalman Filter Integration for Base Models\n",
    "# -----------------\n",
    "\n",
    "# Random Forest Integration with Kalman Filters\n",
    "rf_predictions_trimmed = rf_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for Random Forest\n",
    "cvkf_params_rf, cvkf_preds_rf = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    rf_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_rf = calculate_classification_metrics(y_test, (cvkf_preds_rf > 0.5).astype(int), cvkf_preds_rf)\n",
    "\n",
    "# FMKF for Random Forest\n",
    "fmkf_params_rf, fmkf_preds_rf = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    rf_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_rf = calculate_classification_metrics(y_test, (fmkf_preds_rf > 0.5).astype(int), fmkf_preds_rf)\n",
    "\n",
    "# XGBoost Integration with Kalman Filters\n",
    "xgb_predictions_trimmed = xgb_preds_proba[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for XGBoost\n",
    "cvkf_params_xgb, cvkf_preds_xgb = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    xgb_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_xgb = calculate_classification_metrics(y_test, (cvkf_preds_xgb > 0.5).astype(int), cvkf_preds_xgb)\n",
    "\n",
    "# FMKF for XGBoost\n",
    "fmkf_params_xgb, fmkf_preds_xgb = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    xgb_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_xgb = calculate_classification_metrics(y_test, (fmkf_preds_xgb > 0.5).astype(int), fmkf_preds_xgb)\n",
    "\n",
    "# -----------------\n",
    "# Kalman Filter Integration for Neural Network Ensemble\n",
    "# -----------------\n",
    "nn_ensemble_predictions_trimmed = nn_ensemble_probs[:len(y_test)]  # Align predictions\n",
    "\n",
    "# CVKF for Neural Network Ensemble\n",
    "cvkf_params_nn_ensemble, cvkf_preds_nn_ensemble = optimize_kalman_hyperparameters(\n",
    "    lambda **params: ConstantVelocityKalmanFilter(**params),\n",
    "    CVKF_PARAM_GRID,\n",
    "    nn_ensemble_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "cvkf_metrics_nn_ensemble = calculate_classification_metrics(\n",
    "    y_test, (cvkf_preds_nn_ensemble > 0.5).astype(int), cvkf_preds_nn_ensemble\n",
    ")\n",
    "\n",
    "# FMKF for Neural Network Ensemble\n",
    "fmkf_params_nn_ensemble, fmkf_preds_nn_ensemble = optimize_kalman_hyperparameters(\n",
    "    lambda **params: FinancialModelKalmanFilter(**params),\n",
    "    FMKF_PARAM_GRID,\n",
    "    nn_ensemble_predictions_trimmed,\n",
    "    y_test,\n",
    "    metric=\"auc\",\n",
    "    n_jobs=1\n",
    ")\n",
    "fmkf_metrics_nn_ensemble = calculate_classification_metrics(\n",
    "    y_test, (fmkf_preds_nn_ensemble > 0.5).astype(int), fmkf_preds_nn_ensemble\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# Summarizing All Metrics\n",
    "# -----------------\n",
    "all_metrics = {\n",
    "    \"T-1 Baseline\": t1_metrics,\n",
    "    \"Random Classifier\": random_metrics,\n",
    "    \"Rolling Naive Bayes\": rolling_nb_metrics,\n",
    "    \"Logistic Regression\": log_reg_metrics,\n",
    "    \"Random Forest\": rf_metrics,\n",
    "    \"XGBoost\": xgb_metrics,\n",
    "    \"Neural Network\": nn_metrics,\n",
    "    \"LSTM\": lstm_metrics,\n",
    "    \"Stacking Ensemble\": stacking_metrics,\n",
    "    \"Weighted Ensemble\": weighted_ensemble_metrics,\n",
    "    \"Lasso Ensemble\": lasso_ensemble_metrics,\n",
    "    \"Neural Network Ensemble\": nn_ensemble_metrics,\n",
    "    # Kalman Filters for Base Models\n",
    "    \"CVKF (Logistic Regression)\": cvkf_metrics_log_reg,\n",
    "    \"FMKF (Logistic Regression)\": fmkf_metrics_log_reg,\n",
    "    \"CVKF (Random Forest)\": cvkf_metrics_rf,\n",
    "    \"FMKF (Random Forest)\": fmkf_metrics_rf,\n",
    "    \"CVKF (XGBoost)\": cvkf_metrics_xgb,\n",
    "    \"FMKF (XGBoost)\": fmkf_metrics_xgb,\n",
    "    \"CVKF (Neural Network)\": cvkf_metrics_nn,\n",
    "    \"FMKF (Neural Network)\": fmkf_metrics_nn,\n",
    "    \"CVKF (LSTM)\": cvkf_metrics_lstm,\n",
    "    \"FMKF (LSTM)\": fmkf_metrics_lstm,\n",
    "    # Kalman Filters for Ensembles\n",
    "    \"CVKF (Stacking)\": cvkf_metrics_stack,\n",
    "    \"FMKF (Stacking)\": fmkf_metrics_stack,\n",
    "    \"CVKF (Weighted)\": cvkf_metrics_weight,\n",
    "    \"FMKF (Weighted)\": fmkf_metrics_weight,\n",
    "    \"CVKF (Lasso)\": cvkf_metrics_lasso,\n",
    "    \"FMKF (Lasso)\": fmkf_metrics_lasso,\n",
    "    \"CVKF (Neural Network Ensemble)\": cvkf_metrics_nn_ensemble,\n",
    "    \"FMKF (Neural Network Ensemble)\": fmkf_metrics_nn_ensemble,\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "print(\"Final Model Metrics:\\n\", metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with ideal pid102234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if xgboost and lasso (base) results are also identical in another patient (pid102234)\n",
    "Xy_102234 = load_and_preprocess_data(ideal_group_dict[('pid102234', 0)])\n",
    "Xy_102234_cleaned = drop_nan_from_Xy(*Xy_102234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Metrics: {'Accuracy': 0.9991547069957184, 'Precision': 0.9957312546399406, 'Recall': 0.9957312546399406, 'F1': 0.9957312546399406, 'AUC': np.float64(0.9990890647080433)}\n",
      "Lasso (Base) Metrics: {'Accuracy': 0.9980153990334258, 'Precision': 0.9844036697247707, 'Recall': 0.9957312546399406, 'F1': 0.9900350618195239, 'AUC': np.float64(0.9988249597163328)}\n"
     ]
    }
   ],
   "source": [
    "xgboost(*Xy_102234_cleaned)\n",
    "lasso_base(*Xy_102234_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
