{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-way split (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.optimize import minimize\n",
    "from kalman_filter.kalman_filter import (\n",
    "    ConstantVelocityKalmanFilter, FinancialModelKalmanFilter, optimize_kalman_hyperparameters\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pywt  # Ensure you have pywavelets installed for wavelet transforms\n",
    "# from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Hyperparameter Configurations\n",
    "# -----------------\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "LASSO_PARAM_GRID = {\"logisticregression__C\": np.logspace(-3, 2, 10)}\n",
    "RF_PARAM_GRID = {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]}\n",
    "XGB_PARAM_GRID = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "NN_PARAM_GRID = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100]\n",
    "}\n",
    "LSTM_PARAM_GRID = {\n",
    "    \"hidden_size\": [32, 64, 128],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    \"num_epochs\": [50, 100]\n",
    "}\n",
    "\n",
    "# Kalman Filter Hyperparameters\n",
    "CVKF_PARAM_GRID = [\n",
    "    {\"initial_state\": np.array([0.0]), \"Q_diag\": [q], \"R_diag\": [r]}\n",
    "    for q in [0.01, 0.1, 1.0, 10.0]\n",
    "    for r in [0.01, 0.1, 1.0, 10.0]\n",
    "]\n",
    "FMKF_PARAM_GRID = [\n",
    "    {\"initial_state\": np.array([0.0]), \"Q_diag\": [q], \"R_diag\": [r], \"alpha\": [a], \"beta\": [b]}\n",
    "    for q in [0.01, 0.1, 1.0, 10.0]\n",
    "    for r in [0.01, 0.1, 1.0, 10.0]\n",
    "    for a in [0.4, 0.6, 0.8, 1.0]\n",
    "    for b in [0.05, 0.1, 0.2, 0.4]\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Utility Functions\n",
    "# -----------------\n",
    "def two_way_split(X, y, train_size=0.6, test_size=0.4):\n",
    "    \"\"\"Split data into training and testing subsets chronologically.\"\"\"\n",
    "    total_len = len(X)\n",
    "\n",
    "    train_len = round(total_len * train_size)\n",
    "    test_len = total_len - train_len\n",
    "\n",
    "    train_idx = range(0, train_len)\n",
    "    test_idx = range(train_len, total_len)\n",
    "\n",
    "    return (\n",
    "        X.iloc[train_idx], X.iloc[test_idx],\n",
    "        y.iloc[train_idx], y.iloc[test_idx]\n",
    "    )\n",
    "\n",
    "# def five_way_split(X, y, train_size=0.5, val1_size=0.15, val2_size=0.1, kalman_size=0.1, test_size=0.15):\n",
    "    # \"\"\"Split data into five subsets.\"\"\"\n",
    "    # total_len = len(X)\n",
    "\n",
    "    # train_len = round(total_len * train_size)\n",
    "    # val1_len = round(total_len * val1_size)\n",
    "    # val2_len = round(total_len * val2_size)\n",
    "    # kalman_len = round(total_len * kalman_size)\n",
    "    # test_len = total_len - train_len - val1_len - val2_len - kalman_len\n",
    "\n",
    "    # train_idx = range(0, train_len)\n",
    "    # val1_idx = range(train_len, train_len + val1_len)\n",
    "    # val2_idx = range(train_len + val1_len, train_len + val1_len + val2_len)\n",
    "    # kalman_idx = range(train_len + val1_len + val2_len, train_len + val1_len + val2_len + kalman_len)\n",
    "    # test_idx = range(train_len + val1_len + val2_len + kalman_len, total_len)\n",
    "\n",
    "    # return (\n",
    "    #     X.iloc[train_idx], X.iloc[val1_idx], X.iloc[val2_idx], X.iloc[kalman_idx], X.iloc[test_idx],\n",
    "    #     y.iloc[train_idx], y.iloc[val1_idx], y.iloc[val2_idx], y.iloc[kalman_idx], y.iloc[test_idx]\n",
    "    # )\n",
    "\n",
    "\n",
    "def optimize_model_hyperparameters(model_fn, param_grid, X_train, y_train, validation_data, n_jobs=1):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter optimization using GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        model_fn: A callable that returns an instance of the model.\n",
    "        param_grid: Dictionary of hyperparameters to search.\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        validation_data: Tuple (X_val, y_val) for validation.\n",
    "        n_jobs: Number of parallel jobs for GridSearchCV.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The best model after GridSearchCV.\n",
    "        best_params: The best parameters from the search.\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics including Accuracy, Precision, Recall, F1, and AUC.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        y_pred_proba (array-like, optional): Predicted probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    if y_pred_proba is not None:\n",
    "        metrics[\"AUC\"] = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def preprocess_data_with_advanced_features(data_frame, target_column, lag_steps=None, rolling_window=10):\n",
    "    \"\"\"\n",
    "    Preprocess data for time series modeling with advanced feature engineering.\n",
    "    Ensures no data leakage by strictly using past and current data for feature generation.\n",
    "\n",
    "    Args:\n",
    "        data_frame (str): Variable name of loaded pandas data frame.\n",
    "        target_column (str): Target column name.\n",
    "        lag_steps (list): List of lag steps for feature engineering.\n",
    "        rolling_window (int): Window size for rolling features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature DataFrame (X) and target series (y).\n",
    "    \"\"\"\n",
    "    # Load data and parse dates\n",
    "    data = data_frame\n",
    "    data.index = pd.to_datetime(data.index, errors='coerce')  # Ensure index is datetime\n",
    "    assert data.index.is_monotonic_increasing, \"Dataset is not sorted by time.\"\n",
    "\n",
    "    # Fill missing values in the target column\n",
    "    data[target_column] = data[target_column].interpolate(method='linear').bfill()\n",
    "\n",
    "    # Initialize feature storage\n",
    "    features = []\n",
    "    indices = []\n",
    "\n",
    "    for end_idx in range(rolling_window, len(data)):\n",
    "        # Define the current window\n",
    "        window = data.iloc[end_idx - rolling_window:end_idx]\n",
    "\n",
    "        # Compute features for the current timestamp\n",
    "        current_features = {}\n",
    "\n",
    "        # Rolling statistics\n",
    "        signal_cols = [col for col in data.columns if col not in ['patient', 'newtest', 'target', 'event1', 'event2', 'event3', 'event4', 'sleepstage']]  # sleepstage excluded as categorical variable\n",
    "        for col in signal_cols:\n",
    "            current_features[f'{col}_roll_mean'] = window[col].mean()\n",
    "            current_features[f'{col}_roll_std'] = window[col].std()\n",
    "\n",
    "        # Lagged features\n",
    "        if lag_steps:\n",
    "            for lag in lag_steps:\n",
    "                if end_idx - lag >= 0:\n",
    "                    current_features[f'{target_column}_lag{lag}'] = data[target_column].iloc[end_idx - lag]\n",
    "\n",
    "        # Fourier Transform Features\n",
    "        for col in signal_cols:\n",
    "            fourier_transform = np.abs(np.fft.fft(window[col].fillna(0)))\n",
    "            current_features[f'{col}_fft_max'] = np.max(fourier_transform)\n",
    "            current_features[f'{col}_fft_mean'] = np.mean(fourier_transform)\n",
    "\n",
    "        # Wavelet Transform Features\n",
    "        for col in signal_cols:\n",
    "            coeffs = pywt.wavedec(window[col].fillna(0), 'db1', level=3)\n",
    "            current_features[f'{col}_wavelet_approx'] = coeffs[0].mean()\n",
    "            current_features[f'{col}_wavelet_detail1'] = coeffs[1].mean()\n",
    "            current_features[f'{col}_wavelet_detail2'] = coeffs[2].mean()\n",
    "\n",
    "        # Add features and corresponding index\n",
    "        features.append(current_features)\n",
    "        indices.append(data.index[end_idx])\n",
    "\n",
    "    # Convert features to DataFrame\n",
    "    feature_df = pd.DataFrame(features, index=indices)\n",
    "\n",
    "    # Align target values\n",
    "    y = data.loc[feature_df.index, target_column]\n",
    "\n",
    "    return feature_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load and Preprocess Data\n",
    "# -----------------\n",
    "\n",
    "def load_and_preprocess_data(dataframe):\n",
    "    # Load and preprocess data with advanced features\n",
    "    X, y = preprocess_data_with_advanced_features(\n",
    "        data_frame=dataframe,\n",
    "        target_column='target',\n",
    "        lag_steps=[1, 2, 3],\n",
    "        rolling_window=10\n",
    "    )\n",
    "\n",
    "    # Perform five-way split\n",
    "    X_train, X_test, y_train, y_test = two_way_split(\n",
    "        X, y, train_size=0.6, test_size=0.4\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "master_df = pd.read_stata('../data/stmary/processed-data/combined-patient-data-1_00.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group master dataframe by 'patient' and 'newtest' pairs (i.e. by each unique patient data)\n",
    "# Access or initialize each dataframe like: group_dict[('pid100100', 0)]\n",
    "group_dict = {\n",
    "    (val1, val2): data\n",
    "    for (val1, val2), data in master_df.groupby(['patient', 'newtest'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframes\n",
    "for group_key, subset_df in group_dict.items():\n",
    "    subset_df['target'] = subset_df[['event1', 'event2', 'event3', 'event4']].apply(lambda x: 1 if 'Hypopnea' in x.values or 'Apnea Obstructive' in x.values or 'Apnea Central' in x.values or 'Apnea Mixed' in x.values else 0, axis=1)\n",
    "\n",
    "    cols = list(subset_df.columns)\n",
    "    cols.remove('target')\n",
    "    cols.insert(3, 'target')\n",
    "    subset_df = subset_df[cols]\n",
    "\n",
    "    subset_df.set_index('timess', inplace=True)\n",
    "    \n",
    "    group_dict[group_key] = subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_before_dropnan = load_and_preprocess_data(group_dict[('pid100816', 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaN from Xs and corresponding rows from ys\n",
    "def drop_nan_from_Xy(X_train, X_test, y_train, y_test):\n",
    "    # Drop rows with NaN from X\n",
    "    X_train_cleaned = X_train.dropna()\n",
    "    X_test_cleaned = X_test.dropna()\n",
    "    \n",
    "    # Drop corresponding rows from y\n",
    "    y_train_cleaned = y_train[X_train_cleaned.index]\n",
    "    y_test_cleaned = y_test[X_test_cleaned.index]\n",
    "    \n",
    "    return (\n",
    "        X_train_cleaned, X_test_cleaned,\n",
    "        y_train_cleaned, y_test_cleaned\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = drop_nan_from_Xy(*Xy_before_dropnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics: {'Accuracy': 0.9997589324149806, 'Precision': 0.9961285327138986, 'Recall': 0.9930528753377075, 'F1': 0.9945883262466177, 'AUC': np.float64(0.9978413416551788)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Logistic Regression\n",
    "# -----------------\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression(class_weight='balanced', max_iter=2000))  # Handle class imbalance\n",
    "])\n",
    "log_reg_grid = GridSearchCV(log_reg_pipeline, LASSO_PARAM_GRID, cv=5, scoring='roc_auc')\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "log_reg_model = log_reg_grid.best_estimator_\n",
    "\n",
    "log_reg_preds = log_reg_model.predict(X_test)\n",
    "log_reg_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, log_reg_preds),\n",
    "    \"Precision\": precision_score(y_test, log_reg_preds, zero_division=0),  # Avoid warning\n",
    "    \"Recall\": recall_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, log_reg_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Logistic Regression Metrics:\", log_reg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-way split (only transitioning 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126788/4138185884.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_transition.drop('target_lag1', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_copy = group_dict[('pid100816', 0)].copy()\n",
    "\n",
    "# Create a lagged target variable\n",
    "df_copy['target_lag1'] = df_copy['target'].shift(1)\n",
    "\n",
    "# Filter the dataset to only observations where previous state of target was 0\n",
    "df_transition = df_copy[df_copy['target_lag1'] == 0]\n",
    "df_transition.drop('target_lag1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126788/2857536397.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[target_column] = data[target_column].interpolate(method='linear').bfill()\n"
     ]
    }
   ],
   "source": [
    "Xy_before_dropnan = load_and_preprocess_data(df_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = drop_nan_from_Xy(*Xy_before_dropnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics: {'Accuracy': 0.9594838351602608, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC': np.float64(0.32683705019406617)}\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Logistic Regression\n",
    "# -----------------\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression(class_weight='balanced', max_iter=2000))  # Handle class imbalance\n",
    "])\n",
    "log_reg_grid = GridSearchCV(log_reg_pipeline, LASSO_PARAM_GRID, cv=5, scoring='roc_auc')\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "log_reg_model = log_reg_grid.best_estimator_\n",
    "\n",
    "log_reg_preds = log_reg_model.predict(X_test)\n",
    "log_reg_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, log_reg_preds),\n",
    "    \"Precision\": precision_score(y_test, log_reg_preds, zero_division=0),  # Avoid warning\n",
    "    \"Recall\": recall_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, log_reg_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Logistic Regression Metrics:\", log_reg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-way split (no lagged features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_with_advanced_features(data_frame, target_column, lag_steps=None, rolling_window=10):\n",
    "    \"\"\"\n",
    "    Preprocess data for time series modeling with advanced feature engineering.\n",
    "    Ensures no data leakage by strictly using past and current data for feature generation.\n",
    "\n",
    "    Args:\n",
    "        data_frame (str): Variable name of loaded pandas data frame.\n",
    "        target_column (str): Target column name.\n",
    "        lag_steps (list): List of lag steps for feature engineering.\n",
    "        rolling_window (int): Window size for rolling features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature DataFrame (X) and target series (y).\n",
    "    \"\"\"\n",
    "    # Load data and parse dates\n",
    "    data = data_frame\n",
    "    data.index = pd.to_datetime(data.index, errors='coerce')  # Ensure index is datetime\n",
    "    assert data.index.is_monotonic_increasing, \"Dataset is not sorted by time.\"\n",
    "\n",
    "    # Fill missing values in the target column\n",
    "    data[target_column] = data[target_column].interpolate(method='linear').bfill()\n",
    "\n",
    "    # Initialize feature storage\n",
    "    features = []\n",
    "    indices = []\n",
    "\n",
    "    for end_idx in range(rolling_window, len(data)):\n",
    "        # Define the current window\n",
    "        window = data.iloc[end_idx - rolling_window:end_idx]\n",
    "\n",
    "        # Compute features for the current timestamp\n",
    "        current_features = {}\n",
    "\n",
    "        # Rolling statistics\n",
    "        signal_cols = [col for col in data.columns if col not in ['patient', 'newtest', 'target', 'event1', 'event2', 'event3', 'event4', 'sleepstage']]  # sleepstage excluded as categorical variable\n",
    "        for col in signal_cols:\n",
    "            current_features[f'{col}_roll_mean'] = window[col].mean()\n",
    "            current_features[f'{col}_roll_std'] = window[col].std()\n",
    "\n",
    "        # # Lagged features\n",
    "        # if lag_steps:\n",
    "        #     for lag in lag_steps:\n",
    "        #         if end_idx - lag >= 0:\n",
    "        #             current_features[f'{target_column}_lag{lag}'] = data[target_column].iloc[end_idx - lag]\n",
    "\n",
    "        # Fourier Transform Features\n",
    "        for col in signal_cols:\n",
    "            fourier_transform = np.abs(np.fft.fft(window[col].fillna(0)))\n",
    "            current_features[f'{col}_fft_max'] = np.max(fourier_transform)\n",
    "            current_features[f'{col}_fft_mean'] = np.mean(fourier_transform)\n",
    "\n",
    "        # Wavelet Transform Features\n",
    "        for col in signal_cols:\n",
    "            coeffs = pywt.wavedec(window[col].fillna(0), 'db1', level=3)\n",
    "            current_features[f'{col}_wavelet_approx'] = coeffs[0].mean()\n",
    "            current_features[f'{col}_wavelet_detail1'] = coeffs[1].mean()\n",
    "            current_features[f'{col}_wavelet_detail2'] = coeffs[2].mean()\n",
    "\n",
    "        # Add features and corresponding index\n",
    "        features.append(current_features)\n",
    "        indices.append(data.index[end_idx])\n",
    "\n",
    "    # Convert features to DataFrame\n",
    "    feature_df = pd.DataFrame(features, index=indices)\n",
    "\n",
    "    # Align target values\n",
    "    y = data.loc[feature_df.index, target_column]\n",
    "\n",
    "    return feature_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load and Preprocess Data\n",
    "# -----------------\n",
    "\n",
    "def load_and_preprocess_data(dataframe):\n",
    "    # Load and preprocess data with advanced features\n",
    "    X, y = preprocess_data_with_advanced_features(\n",
    "        data_frame=dataframe,\n",
    "        target_column='target',\n",
    "        lag_steps=[1, 2, 3],\n",
    "        rolling_window=10\n",
    "    )\n",
    "\n",
    "    # Perform five-way split\n",
    "    X_train, X_test, y_train, y_test = two_way_split(\n",
    "        X, y, train_size=0.6, test_size=0.4\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_before_dropnan = load_and_preprocess_data(group_dict[('pid100816', 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = drop_nan_from_Xy(*Xy_before_dropnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Logistic Regression\n",
    "# -----------------\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression(class_weight='balanced', max_iter=2000))  # Handle class imbalance\n",
    "])\n",
    "log_reg_grid = GridSearchCV(log_reg_pipeline, LASSO_PARAM_GRID, cv=5, scoring='roc_auc')\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "log_reg_model = log_reg_grid.best_estimator_\n",
    "\n",
    "log_reg_preds = log_reg_model.predict(X_test)\n",
    "log_reg_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, log_reg_preds),\n",
    "    \"Precision\": precision_score(y_test, log_reg_preds, zero_division=0),  # Avoid warning\n",
    "    \"Recall\": recall_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"F1\": f1_score(y_test, log_reg_preds, zero_division=0),\n",
    "    \"AUC\": roc_auc_score(y_test, log_reg_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "print(\"Logistic Regression Metrics:\", log_reg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
